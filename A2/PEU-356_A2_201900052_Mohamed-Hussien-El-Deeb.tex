\documentclass[12pt]{article}
\usepackage[svgnames,x11names,table]{xcolor}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[thicklines]{cancel}

\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=black,
    urlcolor=RoyalBlue4,
}

\title{PEU 356 Assignment 2}
\author{Mohamed Hussien El-Deeb (201900052)}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\hypersetup{linkcolor=RoyalBlue4}

\newcommand*{\Perm}[2]{{}^{#1}\!P_{#2}}
\newcommand*{\Comb}[2]{{}^{#1} C_{#2}}

\newpage
\section{4.1.1}

\subsection{Problem}

Show that if all the components of any tensor of any rank vanish in one particular
coordinate system, they vanish in all coordinate systems.

\textit{Note}. This point takes on special importance in the four-dimensional (4-D) curved
space of general relativity. If a quantity, expressed as a tensor, exists in one coordinate
system, it exists in all coordinate systems and is not just a consequence of a \textbf{choice}
of a coordinate system (as are centrifugal and Coriolis forces in Newtonian mechanics).

\subsection{Solution}

The transformation law for a mixed tensor of arbitrary rank is given by

\[
    {\left(\textbf{T}^\prime\right)}^{\alpha \ldots}{ }_{\beta \ldots}
    = \textbf{T}^{i \ldots}{ }_{j \ldots}
    \frac{\partial {\left(x^\prime\right)}^\alpha}{\partial x^i}
    \frac{\partial x^j}{\partial {\left(x^\prime\right)}^\beta} \cdots
\]

If all the components of a tensor vanish in one particular coordinate system, then
the multiplication of the components by the transformation law will also vanish in
the new coordinate system. Therefore, the components of the tensor will vanish
in all coordinate systems.

\[
    \textbf{0} = \textbf{0}
    \frac{\partial {\left(x^\prime\right)}^\alpha}{\partial x^i}
    \frac{\partial x^j}{\partial {\left(x^\prime\right)}^\beta} \cdots
\]

\newpage
\section{4.1.2}

\subsection{Problem}

The components of tensor \textbf{A} are equal to the corresponding components
of tensor \textbf{B} in one particular coordinate system denoted, by the
superscript 0; that is,

\[
    \textbf{A}^0_{ij} = \textbf{B}^0_{ij}.
\]

Show that tensor \textbf{A} is equal to tensor \textbf{B},
\(\textbf{A}_{ij} = \textbf{B}_{ij}\), in all coordinate systems.

\subsection{Solution}

The transformation law for a covariant tensor of second rank is given by

\[
    \textbf{A}^0_{ij} =
    \frac{\partial x^\alpha}{\partial {\left(x^0\right)}^i}
    \frac{\partial x^\beta}{\partial {\left(x^0\right)}^j}
    \textbf{A}_{\alpha \beta}
\]

\[
    \textbf{B}^0_{ij} =
    \frac{\partial x^\alpha}{\partial {\left(x^0\right)}^i}
    \frac{\partial x^\beta}{\partial {\left(x^0\right)}^j}
    \textbf{B}_{\alpha \beta}
\]

\[
    \because \textbf{A}^0_{ij} = \textbf{B}^0_{ij}
\]

\[
    \therefore
    \frac{\partial x^\alpha}{\partial {\left(x^0\right)}^i}
    \frac{\partial x^\beta}{\partial {\left(x^0\right)}^j}
    \textbf{A}_{\alpha \beta} =
    \frac{\partial x^\alpha}{\partial {\left(x^0\right)}^i}
    \frac{\partial x^\beta}{\partial {\left(x^0\right)}^j}
    \textbf{B}_{\alpha \beta}
\]

\[
    \implies \textbf{A}_{\alpha \beta} = \textbf{B}_{\alpha \beta} \equiv
    \textbf{A}_{ij} = \textbf{B}_{ij}
\]

\newpage
\section{4.1.5}

\subsection{Problem}

The 4-D fourth-rank Riemann-Christoffel curvature tensor of general relativity,
\(\textbf{R}_{iklm}\), satisfies the symmetry relations

\[
    \textbf{R}_{iklm} = - \textbf{R}_{ikml} = - \textbf{R}_{kilm}
\]

With the indices running from 0 to 3, show that the number of independent components
is reduced from 256 to 36 and that the condition

\[
    \textbf{R}_{iklm} = \textbf{R}_{lmik}
\]

further reduces the number of independent components to 21. Finally, if the components
satisfy an identity \(\textbf{R}_{iklm} + \textbf{R}_{ilmk} + \textbf{R}_{imkl} = 0\),
show that the number of independent components is reduced to 20.

\textit{Note}. The final three-term identity furnishes new
information only if all four indices are different.

\subsection{Solution}

Given identities,

\[
    \label{e1} \textbf{[1]} \quad \textbf{R}_{iklm} = - \textbf{R}_{kilm}
\]

\[
    \label{e2} \textbf{[2]} \quad \textbf{R}_{iklm} = - \textbf{R}_{ikml}
\]

\[
    \label{e3} \textbf{[3]} \quad \textbf{R}_{iklm} = \textbf{R}_{lmik}
\]

\[
    \label{e4} \textbf{[4]} \quad \textbf{R}_{iklm} = - (\textbf{R}_{ilmk} + \textbf{R}_{imkl})
\]

Number of dimensions,

\[
    n = 4
\]

General number of indices,

\[
    n^4 = 4^4 = 256
\]

We could treat \(\textbf{R}_{iklm}\) as a 4-D matrix of 4-D matrices,

\[
    \textbf{R}_{iklm}
    \equiv \textbf{R}_{AB} \quad \text{where} \quad A_{ik} \quad \text{and} \quad B_{lm}
\]

Each of the 4-D matrices \(A, B\) is antisymmetric and thus has 6 independent components, this follows
from identities [\hyperref[e1]{1}] and [\hyperref[e2]{2}],

\[
    N(A) = N(B) = N = n(n-1)/2 = 4(4-1)/2 = 6
\]

If we stop here we get \(N(A) * N(B) = 6 * 6 = 36\) independent components. However, we can further reduce the number
of independent components by using identity [\hyperref[e3]{3}],

\[
    N[N+1]/2 = 6(6+1)/2 = 21
\]

Since the final identity [\hyperref[e4]{4}] furnishes new information only if all four indices are different we need to only
consider the \(\Comb{4}{4} = 1\) constraint.

Subtracting that from 21 results in a final count of 20 independent components.

\newpage
\section{4.1.6}

\subsection{Problem}

\(\textbf{T}_{iklm}\) is antisymmetric with respect to all pairs of indices. How many
independent component has it (in 3-D space)?

\subsection{Solution}

\[
    - \textbf{T}_{iklm}
    = \textbf{T}_{kilm}
    = \textbf{T}_{lkim}
    = \textbf{T}_{mkli}
    = \textbf{T}_{ilkm}
    = \textbf{T}_{imlk}
    = \textbf{T}_{ikml}
\]

Since we are in 3-D space, a 4th rank tensor is bound to have a repeated index which results in
forcing the whole tensor to be 0 meaning that we have 0 independent components.

\newpage
\section{4.1.7}

\subsection{Problem}

If \(\textbf{T}_{\ldots i}\) is a tensor of rank n, show that
\(\partial \textbf{T}_{\ldots i} / \partial x^j\) is a tensor of rank \(n + 1\)
(Cartesian coordinates).

\textit{Note}. In non-Cartesian coordinate systems the coefficients \(a_{ij}\) are, in general,
functions of the coordinates, and the derivatives the components of a tensor of rank \(n\) do
not form a tensor except in the special case \(n = 0\). In this case the derivative does yield a
covariant vector (tensor of rank 1).

\subsection{Solution}

\[
    {\left(\textbf{T}^\prime\right)}_{\ldots i}
    = \underbrace{\cdots
        \frac{\partial x^\alpha}{\partial {\left(x^\prime\right)}^i}}_n
    \textbf{T}_{\ldots \alpha}
\]

\[
    \frac{\partial \left({\left(\textbf{T}^\prime\right)}_{\ldots i}\right)}{\partial {\left(x^\prime\right)}^j}
    = \frac{\partial \left(
        \cdots
        \frac{\partial x^\alpha}{\partial {\left(x^\prime\right)}^i}
        \textbf{T}_{\ldots \alpha}
        \right)}{\partial {\left(x^\prime\right)}^j}
    = \cdots
    \frac{\partial x^\alpha}{\partial {\left(x^\prime\right)}^i}
    \frac{\partial \left(
        \textbf{T}_{\ldots \alpha}
        \right)}{\partial {\left(x^\prime\right)}^j}
    + \frac{\partial \left(
        \cdots
        \frac{\partial x^\alpha}{\partial {\left(x^\prime\right)}^i}
        \right)}{\partial {\left(x^\prime\right)}^j}
    \textbf{T}_{\ldots \alpha}
\]

Since we are in Cartesian \(\frac{\partial x^i}{\partial x^j} = \delta^i_j\),

\[
    = \cdots
    \frac{\partial x^\alpha}{\partial {\left(x^\prime\right)}^i}
    \frac{\partial x^\beta}{\partial {\left(x^\prime\right)}^j}
    \frac{\partial \left(
        \textbf{T}_{\ldots \alpha}
        \right)}{\partial x^\beta}
    +
    \frac{\partial x^\beta}{\partial {\left(x^\prime\right)}^j}
    \underbrace{\frac{\partial \left(
            \cdots
            \frac{\partial x^\alpha}{\partial {\left(x^\prime\right)}^i}
            \right)}{\partial x^\beta}
    }_{
        \cancelto{0}{\cdots + \cdots
            \frac{\partial}{\partial {\left(x^\prime\right)}^i}
            \left(\frac{\partial x^\alpha}{\partial x^\beta}\right)}
    }
    \textbf{T}_{\ldots \alpha}
\]

\[
    \frac{\partial \left({\left(\textbf{T}^\prime\right)}_{\ldots i}\right)}{\partial {\left(x^\prime\right)}^j}
    = \underbrace{
        \cdots
        \frac{\partial x^\alpha}{\partial {\left(x^\prime\right)}^i}
        \frac{\partial x^\beta}{\partial {\left(x^\prime\right)}^j}}_{n + 1}
    \frac{\partial \left(
        \textbf{T}_{\ldots \alpha}
        \right)}{\partial x^\beta}
\]

\newpage
\section{4.1.8}

\subsection{Problem}

If \(\textbf{T}_{ijk\ldots}\) is a tensor of rank \(n\), show that
\(\sum_j \partial \textbf{T}_{ijk\ldots}/\partial x^j\) is a tensor of rank \(n - 1\)
(Cartesian coordinates).

\subsection{Solution}

\[
    {\left(\textbf{T}^\prime\right)}_{ijk \ldots}
    = \underbrace{
        \frac{\partial x^\alpha}{\partial {\left(x^\prime\right)}^i}
        \frac{\partial x^\beta}{\partial {\left(x^\prime\right)}^j}
        \frac{\partial x^\gamma}{\partial {\left(x^\prime\right)}^k}
        \cdots
    }_n
    \textbf{T}_{\alpha \beta \gamma \ldots}
\]

\[
    \sum_j
    \frac
    {\partial \left({\left(\textbf{T}^\prime\right)}_{ijk \ldots}\right)}
    {\partial {\left(x^\prime\right)}^j}
    = \sum_j
    \frac
    {\partial \left(
        \frac{\partial x^\alpha}{\partial {\left(x^\prime\right)}^i}
        \frac{\partial x^\beta}{\partial {\left(x^\prime\right)}^j}
        \frac{\partial x^\gamma}{\partial {\left(x^\prime\right)}^k}
        \cdots
        \textbf{T}_{\alpha \beta \gamma \ldots}\right)}
    {\partial {\left(x^\prime\right)}^j}
\]

\[
    = \sum_j
    \left(
    \frac{\partial x^\alpha}{\partial {\left(x^\prime\right)}^i}
    \frac{\partial x^\beta}{\partial {\left(x^\prime\right)}^j}
    \frac{\partial x^\gamma}{\partial {\left(x^\prime\right)}^k}
    \cdots \frac
    {\partial \left(\textbf{T}_{\alpha \beta \gamma \ldots}\right)}
    {\partial {\left(x^\prime\right)}^j}
    + \cancelto{0}{\frac{\partial
            \left(
            \frac{\partial x^\alpha}{\partial {\left(x^\prime\right)}^i}
            \frac{\partial x^\beta}{\partial {\left(x^\prime\right)}^j}
            \frac{\partial x^\gamma}{\partial {\left(x^\prime\right)}^k}
            \cdots
            \right)}
        {\partial {\left(x^\prime\right)}^j}}
    \textbf{T}_{\alpha \beta \gamma \ldots}
    \right)
\]

The second term was cancelled using the same argument from the previous question,

\[
    \sum_j
    \frac
    {\partial \left({\left(\textbf{T}^\prime\right)}_{ijk \ldots}\right)}
    {\partial {\left(x^\prime\right)}^j}
    = \frac{\partial x^\alpha}{\partial {\left(x^\prime\right)}^i}
    \frac{\partial x^\gamma}{\partial {\left(x^\prime\right)}^k}
    \cdots
    \sum_j
    \left(\cancelto{\delta^\eta_j}{\sum_\eta
        \frac{\partial x^\beta}{\partial {\left(x^\prime\right)}^j}
        \frac{\partial x^\eta}{\partial {\left(x^\prime\right)}^j}}\right)
    \frac
    {\partial \left(\textbf{T}_{\alpha \beta \gamma \ldots}\right)}
    {\partial x^\eta}
\]

\[
    \sum_j
    \frac
    {\partial \left({\left(\textbf{T}^\prime\right)}_{ijk \ldots}\right)}
    {\partial {\left(x^\prime\right)}^j}
    = \underbrace{\frac{\partial x^\alpha}{\partial {\left(x^\prime\right)}^i}
        \frac{\partial x^\gamma}{\partial {\left(x^\prime\right)}^k}
        \cdots}_{n - 1}
    \sum_j
    \frac
    {\partial \left(\textbf{T}_{\alpha \beta \gamma \ldots}\right)}
    {\partial x^j}
\]

\newpage
\section{4.1.9}

\subsection{Problem}

The operator

\[
    \nabla^2 - \frac{1}{c^2} \frac{\partial^2}{\partial t^2}
\]

may be written as

\[
    \sum^4_{i = 1} \frac{\partial^2}{\partial {x_i}^2},
\]

using \(x_4 = ict\). This is the 4-D Laplacian, sometimes called the d'Alember\-tian and denoted
by \(\Box^2\). Show that it is a \textbf{scalar} operator, that is, invariant under Lorentz
transformations, i.e., under rotations in the space of vectors \((x^1, x^2, x^3, x^4)\).

\subsection{Solution}

\[
    \Box^2 = \sum^4_{i = 1} \frac{\partial^2}{\partial {x_i}^2}
\]

\[
    \left(\Box^\prime\right)^2
    = \sum^4_{i = 1} \frac{\partial^2}{\partial {{\left(x^\prime\right)}_i}^2}
\]

\[
    = \sum^4_{i = 1}
    \left(\cancelto{\delta^i_j}{\sum^4_{j = 1} \frac{\partial {x_j}}{\partial {{\left(x^\prime\right)}_i}}
        \frac{\partial {x_j}}{\partial {{\left(x^\prime\right)}_i}}}\right)
    \frac{\partial^2}{\partial {x_j}^2}
\]

\[
    \left(\Box^\prime\right)^2
    = \sum^4_{i = 1} \frac{\partial^2}{\partial {x_i}^2}
    = \Box^2
\]

Another solution is to realize that \(\Box^2\) is \(\partial^\mu \cdot \partial_\mu = \left(\frac{1}{c} \frac{\partial}{\partial t},-\nabla\right) \cdot \left(\frac{1}{c} \frac{\partial}{\partial t}, \nabla\right)\)

\newpage
\bibliographystyle{plain}
\bibliography{references}
\nocite{arfken2013mathematical}
\nocite{El-Deeb_PEU-356_Assignments}

\end{document}